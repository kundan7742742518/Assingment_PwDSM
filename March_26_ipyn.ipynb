{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## March 26 Machine learning Assinment"
      ],
      "metadata": {
        "id": "rHqB_jiQ3Pva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lenear regresion , Multiple regresion, polynomial regresion."
      ],
      "metadata": {
        "id": "5xXSr1Q83lR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 1"
      ],
      "metadata": {
        "id": "dVmg9lXa3zFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- Simple linear regression involves only one independent variable to predict the dependent variable. It assumes a linear relationship between the independent variable and the dependent variable. The equation for simple linear regression is:\n",
        "Y = β₀ + β₁X + ε\n",
        "\n",
        "\n",
        "Example of Simple Linear Regression:-\n",
        "Let's consider a simple example where we want to predict a student's exam score based on the number of hours they studied. Here, the dependent variable (Y) is the exam score, and the independent variable (X) is the number of hours studied. We collect data for several students and fit a simple linear regression model to predict the exam score based on the hours studied.\n",
        "\n",
        ":-Multiple Linear Regression:\n",
        "Multiple linear regression involves more than one independent variable to predict the dependent variable. It assumes a linear relationship between the dependent variable and a combination of independent variables. The equation for multiple linear regression is:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε\n",
        "\n",
        "\n",
        "Example:-\n",
        "Along with the number of hours studied, we can also include variables like the student's previous exam score, the number of assignments completed, and the amount of sleep they got. By incorporating these variables into the regression model, we can create a multiple linear regression model to predict the exam score based on multiple factors.\n"
      ],
      "metadata": {
        "id": "ElaYEWKx34mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 2"
      ],
      "metadata": {
        "id": "RMSYJSR35aTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression makes several assumptions about the data in order for the results to be valid. These assumptions include:\n",
        "\n",
        "1. Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear. This means that the effect of the independent variables on the dependent variable is additive and proportional.\n",
        "\n",
        "2. Independence: The observations in the dataset should be independent of each other. This assumption implies that there should be no correlation or dependence between the residuals (the differences between the actual and predicted values) of the regression model.\n",
        "\n",
        "3. Homoscedasticity: Homoscedasticity assumes that the residuals have constant variance across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the independent variables.\n",
        "\n",
        "4. Normality: The residuals are assumed to follow a normal distribution. This assumption implies that the errors should be normally distributed with a mean of zero.\n",
        "\n",
        "5. No Multicollinearity: Multicollinearity refers to a high correlation between independent variables in the regression model. It is important to ensure that the independent variables are not too highly correlated with each other, as it can lead to unstable estimates and difficulties in interpreting the model.\n",
        "\n",
        "To check whether these assumptions hold in a given dataset, several diagnostic techniques can be used:\n",
        "\n",
        "1. Residual Analysis: Plotting the residuals against the predicted values or the independent variables can help identify any patterns or deviations from linearity. If there is a clear pattern or curvature in these plots, it suggests a violation of the linearity assumption.\n",
        "\n",
        "2. Independence: Checking for autocorrelation among the residuals can be done using techniques such as the Durbin-Watson test or by plotting the residuals against the order of observations. If autocorrelation is present, it indicates a violation of the independence assumption.\n",
        "\n",
        "3. Homoscedasticity: Plotting the residuals against the predicted values or the independent variables can help identify heteroscedasticity. If the spread of the residuals increases or decreases systematically with the predicted values or the independent variables, it indicates a violation of homoscedasticity.\n",
        "\n",
        "4. Normality: A normality plot, such as a histogram or a Q-Q plot of the residuals, can be used to visually assess the normality assumption. Statistical tests like the Shapiro-Wilk test or the Kolmogorov-Smirnov test can also be employed to formally test the normality of the residuals.\n",
        "\n",
        "5. Multicollinearity: To assess multicollinearity, one can calculate the correlation matrix among the independent variables and look for high correlations. Variance Inflation Factor (VIF) values can also be computed to quantify the extent of multicollinearity.\n",
        "\n",
        "It's important to note that these diagnostic techniques provide indications rather than definitive proof of violations. If assumptions are significantly violated, alternative regression techniques or data transformations may be necessary."
      ],
      "metadata": {
        "id": "U-bBxFE65eof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 3"
      ],
      "metadata": {
        "id": "nTJ7TuAGB_a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a linear regression model, the slope and intercept are coefficients that describe the relationship between the independent variable(s) and the dependent variable.\n",
        "\n",
        "1. Intercept (β₀):\n",
        "The intercept (β₀) represents the estimated value of the dependent variable when all independent variables are equal to zero. It is the point where the regression line intersects the y-axis. The intercept is useful for understanding the baseline or starting point of the relationship between the variables.\n",
        "\n",
        "For example, in a linear regression model predicting housing prices, the intercept might represent the estimated price of a house when all independent variables, such as the size, location, and number of bedrooms, are zero. However, it is important to note that the interpretation of the intercept depends on the context and the scaling of the variables.\n",
        "\n",
        "2. Slope (β₁):\n",
        "The slope (β₁) represents the estimated change in the dependent variable for a one-unit increase in the independent variable, holding other independent variables constant. It indicates the direction and magnitude of the effect of the independent variable on the dependent variable.\n",
        "\n",
        "Continuing with the housing price example, suppose we have a linear regression model with the independent variable \"size of the house\" and the dependent variable \"price.\" If the slope coefficient (β₁) is 2000, it means that, on average, for every one-unit increase in the size of the house (e.g., one square foot), we expect the price of the house to increase by $2000, assuming all other variables remain constant.\n",
        "\n",
        "It is important to interpret the slope in the context of the specific variables and units used. The interpretation may vary depending on the nature of the variables (continuous, categorical) and their scaling.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wj7If10RCQs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS= 4"
      ],
      "metadata": {
        "id": "xLOjXiZkCt4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is an optimization algorithm commonly used in machine learning to minimize the cost function or error of a model. It is an iterative process that adjusts the parameters of the model by following the negative gradient direction to find the optimal values that minimize the cost.\n",
        "\n",
        "The main idea behind gradient descent is to update the model's parameters in small steps by computing the gradient of the cost function with respect to the parameters. The gradient represents the direction of steepest ascent, and by negating it, we move in the direction of steepest descent. This process is repeated iteratively until a minimum of the cost function is reached.\n",
        "\n",
        "The steps involved in gradient descent are as follows:\n",
        "\n",
        "Initialize the parameters: Start by initializing the model's parameters with random or predetermined values.\n",
        "\n",
        "Compute the cost: Evaluate the cost function using the current parameter values. The cost function measures the discrepancy between the model's predictions and the actual values.\n",
        "\n",
        "Compute the gradients: Calculate the partial derivatives of the cost function with respect to each parameter. These gradients represent the direction and magnitude of the steepest ascent in the parameter space.\n",
        "\n",
        "Update the parameters: Adjust the parameters by taking small steps in the negative gradient direction. This is done by multiplying the gradients by a learning rate, which determines the step size.\n",
        "\n",
        "Repeat steps 2 to 4: Iterate the process by recalculating the cost, gradients, and updating the parameters. This loop continues until a stopping criterion is met, such as reaching a specified number of iterations or achieving a desired level of convergence."
      ],
      "metadata": {
        "id": "mOelBXj-DWzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 5"
      ],
      "metadata": {
        "id": "vDuGkbJsDlF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple linear regression is a statistical model that aims to predict the value of a dependent variable based on two or more independent variables. It extends the concept of simple linear regression, which only considers a single independent variable, by incorporating multiple predictors.\n",
        "\n",
        "The multiple linear regression model can be represented by the following equation:\n",
        "\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε\n",
        "\n",
        "\n",
        "\n",
        "The key difference is that multiple linear regression accounts for the combined effects of multiple independent variables on the dependent variable. It allows for a more comprehensive analysis of the relationship between the predictors and the response variable by considering their simultaneous influence. Simple linear regression, on the other hand, focuses on the relationship between a single predictor and the response variable, disregarding the impact of other variables.\n"
      ],
      "metadata": {
        "id": "7QQ0E4kmDv78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 6\n"
      ],
      "metadata": {
        "id": "IcriBmPvECyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity refers to a high correlation or linear relationship between two or more independent variables in a multiple linear regression model. It occurs when the independent variables are not independent of each other, making it difficult to separate their individual effects on the dependent variable.\n",
        "\n",
        "Detecting multicollinearity:\n",
        "There are several methods to detect multicollinearity in a multiple linear regression model:\n",
        "\n",
        "1. Correlation matrix: Calculate the correlation coefficients between all pairs of independent variables. Correlation values close to +1 or -1 indicate a strong linear relationship.\n",
        "\n",
        "2. Variance Inflation Factor (VIF): VIF quantifies the severity of multicollinearity by calculating the inflation of the variances of the regression coefficients. Higher VIF values (typically above 5 or 10) indicate stronger multicollinearity.\n",
        "\n",
        "3. Tolerance: Tolerance is the reciprocal of the VIF and measures the proportion of variance in an independent variable that is not predictable from the other independent variables. Lower tolerance values (typically below 0.2 or 0.1) suggest stronger multicollinearity.\n",
        "\n",
        "Addressing multicollinearity:\n",
        "If multicollinearity is detected in a multiple linear regression model, several approaches can be used to address the issue:\n",
        "\n",
        "1. Remove one or more correlated variables: If two or more variables are highly correlated, removing one of them can help alleviate multicollinearity. This approach should be based on prior knowledge, domain expertise, or theoretical considerations.\n",
        "\n",
        "2. Data collection: Collecting more data can help reduce multicollinearity. Increasing the sample size can provide a more diverse range of observations, potentially reducing the correlation between variables.\n",
        "\n",
        "3. Feature selection: Use feature selection techniques, such as stepwise regression, to automatically select a subset of relevant variables. These methods consider the importance of variables based on statistical criteria, effectively choosing a subset of independent variables with the most impact on the dependent variable.\n",
        "\n",
        "4. Regularization techniques: Regularization methods, such as ridge regression and lasso regression, can help mitigate multicollinearity by introducing a penalty term that shrinks the regression coefficients. These methods encourage variable shrinkage and can reduce the impact of highly correlated variables.\n",
        "\n",
        "5. Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that can be used to create a smaller set of uncorrelated variables, known as principal components, from the original set of correlated variables. These principal components can then be used as predictors in the regression model.\n",
        "\n",
        "6. Domain knowledge and context: Understanding the domain and the variables involved can provide insights into the relationships and potential sources of multicollinearity. Adjusting the model or data collection process based on this knowledge can help address the issue.\n",
        "\n",
        "It is important to note that addressing multicollinearity depends on the specific context, data, and goals of the analysis. It is recommended to carefully consider the options and choose the most appropriate approach based on the particular situation."
      ],
      "metadata": {
        "id": "c24xa2eIEtHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 7"
      ],
      "metadata": {
        "id": "PblQlueeFPhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a form of regression analysis that models the relationship between the independent variable(s) and the dependent variable as an nth-degree polynomial. It extends the linear regression model by allowing for nonlinear relationships between the variables.\n",
        "\n",
        "In polynomial regression, the relationship between the independent variable(s) and the dependent variable is expressed by a polynomial equation of the form:\n",
        "\n",
        "Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε\n",
        "\n",
        "where:\n",
        "- Y is the dependent variable.\n",
        "- X is the independent variable.\n",
        "- β₀, β₁, β₂, ..., βₙ are the coefficients that represent the weights or effects of each term in the polynomial equation.\n",
        "- ε is the error term.\n",
        "\n",
        "The key difference between polynomial regression and linear regression is the inclusion of higher-degree polynomial terms (X², X³, etc.) in the equation. Linear regression assumes a linear relationship between the independent and dependent variables, represented by a straight line. In contrast, polynomial regression allows for more complex curved relationships that can better fit the data.\n",
        "\n",
        "By including higher-degree polynomial terms, polynomial regression can capture nonlinear patterns and curvature in the data. It can be used to model relationships where the relationship between the independent and dependent variables is not linear but exhibits a more curved or U-shaped pattern.\n",
        "\n",
        "For example, consider a dataset where the dependent variable represents the price of a product, and the independent variable represents the quantity sold. In linear regression, the relationship between price and quantity would be assumed to be a straight line. However, if there is a nonlinear relationship, such as an inverted U-shaped pattern, polynomial regression can capture this relationship by including quadratic terms (X²) in the equation.\n",
        "\n",
        "It is important to note that while polynomial regression allows for more flexibility in capturing nonlinear relationships, it can also be more prone to overfitting the data. Overfitting occurs when the model fits the noise or random fluctuations in the data, leading to poor generalization to new data. To mitigate overfitting, regularization techniques or feature selection methods can be employed in polynomial regression.\n",
        "\n",
        "In summary, polynomial regression extends linear regression by incorporating higher-degree polynomial terms in the equation, allowing for modeling of nonlinear relationships between the independent and dependent variables. It provides a flexible approach to capturing curvature and nonlinear patterns in the data."
      ],
      "metadata": {
        "id": "Ig201aobFSGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS = 8"
      ],
      "metadata": {
        "id": "KC3gOxvYF6mW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of Polynomial Regression compared to Linear Regression:\n",
        "\n",
        "1. Nonlinear Relationships: Polynomial regression can capture nonlinear relationships between the independent and dependent variables. It allows for modeling curved patterns, U-shaped or inverted U-shaped relationships, and other complex nonlinearities that linear regression cannot accommodate.\n",
        "\n",
        "2. Flexibility: By including higher-degree polynomial terms, polynomial regression provides greater flexibility in fitting the data. It can provide a better fit and more accurately represent the underlying relationship between the variables when the relationship is nonlinear.\n",
        "\n",
        "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
        "\n",
        "1. Overfitting: Polynomial regression is more susceptible to overfitting the data. When higher-degree polynomial terms are included, the model can become overly complex and fit the noise or random fluctuations in the data, leading to poor generalization to new data.\n",
        "\n",
        "2. Interpretability: The interpretation of the coefficients in polynomial regression becomes more challenging as the degree of the polynomial increases. The coefficients may not have a direct, intuitive interpretation, making it more difficult to explain the relationship between the variables.\n",
        "\n",
        "Situations for Preferably Using Polynomial Regression:\n",
        "\n",
        "1. Nonlinear Relationships: When there is prior knowledge or evidence that suggests a nonlinear relationship between the independent and dependent variables, polynomial regression is a suitable choice. It can capture the curvature and nonlinear patterns in the data, providing a more accurate representation of the relationship.\n",
        "\n",
        "2. Flexibility in Modeling: If the relationship between the variables is expected to be more complex than a simple straight line, polynomial regression offers the flexibility to model the data with higher-degree polynomial terms. This can be beneficial when there are complex interactions or curvature that linear regression cannot adequately capture.\n",
        "\n",
        "3. Limited Sample Size: In situations with a small sample size, polynomial regression may be preferred over more complex nonlinear modeling techniques. It allows for modeling nonlinearities without requiring a large number of observations.\n",
        "\n",
        "It is important to note that the choice between linear regression and polynomial regression depends on the specific context, available data, and the underlying relationship between the variables. Polynomial regression should be used judiciously, considering the trade-off between model complexity, overfitting, and interpretability."
      ],
      "metadata": {
        "id": "M7wYuXL4GNQH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHYRQw1I5dNX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}